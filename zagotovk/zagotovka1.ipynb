{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import pandas as pd\n",
    "import io\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pymorphy2\n",
    "import fitz\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128d381",
   "metadata": {},
   "source": [
    "Изначальные признаки: \n",
    "\n",
    "__TitleCompany__ - Название компании\n",
    "\n",
    "__Description__ - Описание компании\n",
    "\n",
    "__Reiting__ - Рейтинг компании\n",
    "\n",
    "__Categories__ - Категории компании\n",
    "\n",
    "__Date__ - Дата опубликования статьи (в будущем удален, как ненужный признак)\n",
    "\n",
    "__TextPost__ - необработанный текст (в будущем удален, как ненужный признак)\n",
    "\n",
    "__TitleCompany__ - Название компании\n",
    "\n",
    "__Description__ - Описание компании\n",
    "\n",
    "__Reiting__ - Рейтинг компании\n",
    "\n",
    "__Categories__ - Категории компании\n",
    "\n",
    "__TextPostLemat__ - обработанный, лематизированный текст \n",
    "\n",
    "__TextPostToken__ - обработанный, токенизированный и лематизированный текст (лематизирован другим способом нежели TextPostLemat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf56c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd252dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#выводим общую информацию о пдф файле\n",
    "pdf_document = r\"C:\\Users\\essww\\OneDrive\\Рабочий стол\\Компании.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "print(\"Исходный документ: \", doc)\n",
    "print(\"\\nКоличество страниц: %i\\n\\n------------------\\n\\n\" % doc.page_count)\n",
    "print(doc.metadata)\n",
    "text_pages = \"\"\n",
    "for current_page in range(len(doc)):\n",
    "    page = doc.load_page(current_page)\n",
    "    page_text = page.get_text(\"text\")\n",
    "    # записываем текст пдф файла\n",
    "    text_pages += page.get_text(\"text\")\n",
    "    print(\"Стр. \", current_page+1, \"\\n\")\n",
    "    print(page_text\n",
    "# Разделяем текст на строки\n",
    "text_pages = text_pages.split(\"\\n\")\n",
    "text_new_pages = text_pages[0:26] + text_pages[39:132]\n",
    "for i in tqdm(range(0,len(text_new_pages),6)):\n",
    "    TitleCompany.append(text_new_pages[i])\n",
    "    Description.append(text_new_pages[i+1])\n",
    "    Reiting.append(text_new_pages[i+4])\n",
    "    Categories.append(text_new_pages[i+3].replace('�', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381792bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagenum = 1\n",
    "for i in tqdm(range(25)):\n",
    "    url = \"https://habr.com/ru/all/page\" + str(pagenum) + '/'\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    pages = soup.find_all('h2', class_='tm-title tm-title_h2')\n",
    "    for i in pages:\n",
    "        if 'companies' in str(i.a.get('href')):\n",
    "            url = 'https://habr.com' + str(i.a.get('href'))\n",
    "            page = requests.get(url)\n",
    "            soup = bs(page.text, 'html.parser')\n",
    "            TitleCompany = soup.find('a', class_='tm-company-snippet__title')\n",
    "            for i in range(20):\n",
    "                if TitleCompany.text in df['TitleCompany'][i]:\n",
    "                    dfHabr[\"TitleCompany\"].append(df['TitleCompany'][i])\n",
    "                    dfHabr[\"Description\"].append(df['Description'][i])\n",
    "                    dfHabr[\"Reiting\"].append(df['Reiting'][i])\n",
    "                    dfHabr[\"Categories\"].append(df['Categories'][i])\n",
    "                    TextPost = soup.find('div', class_='tm-article-body')\n",
    "                    TextPost.text.replace('\\n', '')\n",
    "                    TextPost.text.replace('\\r', '')\n",
    "                    dfHabr[\"TextPost\"].append(TextPost.text)\n",
    "    pagenum = pagenum + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d51899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод удаления пунктуации\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
    "\n",
    "# метод удаления чисел\n",
    "def remove_numbers(text):\n",
    "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
    "\n",
    "# метод удаления двойных пробелов\n",
    "def remove_multiple_spaces(text):\n",
    "\treturn re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "# метод удаления оставшихся чисел\n",
    "def remove_notalpha(text):\n",
    "    return ''.join([i if i.isalpha() else ' ' for i in text])\n",
    "\n",
    "def remove_english_word(text):\n",
    "    return ''.join(re.sub(r\"[^а-яА-Я\\s]+\", \"\",text))\n",
    "\n",
    "# метод удаления оставшихся символов\n",
    "st = '❯\\xa0'\n",
    "def remove_othersymbol(text):\n",
    "    return ''.join([ch if ch not in st else ' ' for ch in text])\n",
    "\n",
    "# метод токенизирования\n",
    "def tokenize(text):\n",
    "    t = word_tokenize(text)\n",
    "    return [token for token in t if token not in russian_stopwords]\n",
    "\n",
    "def remove_small_text(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([i if len(i) >=3 else ' ' for i in words])\n",
    "\n",
    "mystem = Mystem() \n",
    "# добавление стопслов\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(['…', '«', '»', '...', 'быть', 'r', 'n', 'а', 'мы', 'с', 'для', 'ещё', 'его', 'также', 'к', 'тем', 'кто', 'чтобы', 'но', 'они', 'будут', 'так', 'где', 'один', 'он ', 'и', 'на', 'но', 'или', 'либо', 'это', 'мб', 'далее', 'дв', 'свой', 'ваш','всё', 'очень', 'её', 'ещё', 'вообще', 'наш', 'который'])\n",
    "# обработка текста с лематизацией для токенизации\n",
    "prep_text = [tokenize(remove_multiple_spaces(remove_small_text(remove_notalpha(remove_othersymbol(remove_english_word(remove_numbers(remove_punctuation(text.lower())))))))) for text in tqdm(dfHabr1['TextPost']) if text not in russian_stopwords]\n",
    "dfHabr1['TextPostToken'] = prep_text\n",
    "dfHabr1.drop(columns=['TextPost'], axis= 1 , inplace= True )\n",
    "lemm_texts_list = []\n",
    "for text in tqdm(dfHabr1['TextPostLemat']):\n",
    "    try:\n",
    "        text_lem = mystem.lemmatize(text)\n",
    "        tokens = [token for token in text_lem if token != ' ' and token not in russian_stopwords]\n",
    "        text = \" \".join(tokens)\n",
    "        lemm_texts_list.append(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "dfHabr1['TextPostLemat'] = lemm_texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем DataFrame в файл\n",
    "dfHabr1.to_csv('Report1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ff38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
